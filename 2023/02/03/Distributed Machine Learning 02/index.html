<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Distributed Machine Learning 02 | HyattDD Blog</title><meta name="author" content="Hyatt.D"><meta name="copyright" content="Hyatt.D"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="继续阅读下去啦！充满动力ing，越看越觉得 ML System 领域有好多内容可以做，而且感觉论文作者们做system的实验不容易啊😆"><link rel="shortcut icon" href="/img/webpic.png"><link rel="canonical" href="http://example.com/2023/02/03/Distributed%20Machine%20Learning%2002/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":999,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Distributed Machine Learning 02',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-03 23:23:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/font.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/touxiang.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">47</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="HyattDD Blog"><span class="site-name">HyattDD Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Distributed Machine Learning 02</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-02-03T15:14:59.000Z" title="Created 2023-02-03 23:14:59">2023-02-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-02-03T15:23:36.104Z" title="Updated 2023-02-03 23:23:36">2023-02-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/System/">System</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Distributed Machine Learning 02"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1>Basics of Distributed Machine Learning</h1>
<p>主要介绍基于分布式环境特点下的核心技术与 ML 重要方法</p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    
flowchart LR
A(Distributed ML)--&gt;A1(Parallelism)
A1--&gt;A11(Data Parallelism)
A1--&gt;A12(Model Parallelism)
A1--&gt;A13(Hybrid Parallelism)

A--&gt;A2(Parameter Sharing)
A2--&gt;A21(Shared-Nothing)
A2--&gt;A22(Shared-Memory)

A--&gt;A3(Synchronization)
A3--&gt;A31(Bulk Synchronous Protocol)
A3--&gt;A32(Asynchronous Protocol)
A3--&gt;A33(Stale Synchronous Protocol)

A--&gt;A4(Communication)
A4--&gt;A41(Lower Numerical Precision)
A4--&gt;A42(Communication Compression)
  </pre></div>
<h1>Anatomy of Distributed Machine Learning</h1>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230121235506.png" alt="Pasted image 20230121235506"></p>
<h3 id="Computing-Parallelism">Computing Parallelism</h3>
<p>整体上来讲，完成一项 DML 任务，需要先优化问题划分为几个子问题来把原本的一个整体优化算法给并行化，这是算法设计层面的问题。</p>
<p>然后要将每个子问题部署在一台机器上或者说一个 worker 结点上，然后将计算任务分解给 worker，这是工程实现上的问题。这种任务的分解表现形式为：比如将 data 分解给各个 worker，或者把 model 划分给各个 worker，也可以将 model 或者 data 放在一个 worker 上都行，只不过不同方案效率不一样呗，我们的目的是设计出比较好的方案。</p>
<h3 id="Parameter-Aggregation">Parameter Aggregation</h3>
<p>每个 worker 都参与了模型的训练，那么训练时参数更新肯定要有人负责统一管理，不然不同节点各自搞自己的梯度下降那最后的模型就乱了。这就需要一个参数聚合机制，在 worker 之间聚合和共享参数。</p>
<h3 id="Worker-Synchronization">Worker Synchronization</h3>
<p>对于分布式机器学习，大多数梯度优化算法都是迭代的，因此需要在迭代过程中同步各个 worker 中的内容。不同的 worker 之间的进度不一致，需要解决 stragglers 和其他 worker 之间的同步问题。</p>
<h3 id="Communication-Optimization">Communication Optimization</h3>
<p>分布式机器学习需要通过网络传输数据，通信成本巨大，如何减少通信损耗呢？目前可以联想到的有：设计一些不需要那么多通信的算法？或者减少每次通信的数据量？再或者对通信可能性进行预判，然后将通信频率高的结点在一个局域范围内共享 memory 之类的？感觉可以有很多可做的点，系统设计果然有点艺术成分。</p>
<h1>Parallelism</h1>
<p>设计并行优化算法有两个原则：</p>
<ul>
<li>每个子问题对于一个 worker 来说更容易解决；</li>
<li>子问题的组合等于原始优化问题</li>
</ul>
<p>然后根据优化问题的划分进行数据和模型在全体 workers 上的分配。如果一个子问题需要整个 dataset，则在每个 worker 中复制 dataset 以供训练；否则，将训练的 dataset 划分为不同部分，每个部分都发送给一个 worker。可以说，分布式 ML 中并行性的关键是如何划分训练数据和模型参数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230122001921.png" alt="Pasted image 20230122001921"></p>
<p>总的来讲，划分方法有三种：data parallelism, model parallelism, and hybrid parallelism，最后一种是前两者的结合或者说一种 trade-off</p>
<h2 id="Data-Parallelism">Data Parallelism</h2>
<p>根据数据的划分方向，有两种方法：水平划分和垂直划分</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230122001937.png" alt="Pasted image 20230122001937"></p>
<h3 id="Horizontal-Partition">Horizontal Partition</h3>
<p>水平划分是最常用的划分方式，其划分结果是得到来自原 dataset 的一个个 subset，每一个 subset 中有若干条组成方式为 <code>&lt;character vector，label&gt;</code> 的数据。水平划分要求每个 worker 都存储一个完整的 model，毕竟每个数据条目都包含了要训练的全部特征，所以肯定不能用不完整的模型进行训练。</p>
<p>每个 partition 是完整的，模型也是完整的，所以每个 worker 计算该部分数据可以得到梯度：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>g</mi><mi>w</mi></msub><mo>=</mo><msup><mi>f</mi><msup><mrow></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msup><mo stretchy="false">(</mo><mi>θ</mi><mo separator="true">;</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>b</mi></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>b</mi></mrow></msub><mo stretchy="false">)</mo><mo>⊆</mo><mo stretchy="false">(</mo><msub><mi>X</mi><mi>w</mi></msub><mo separator="true">,</mo><msub><mi>Y</mi><mi>w</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">g_{w}=f^{&#x27;}(\theta; (x_{i:i+b},y_{i:i+b})\subseteq(X_{w},Y_{w}))
\tag{1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2425em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9925em;"><span style="top:-2.9925em;margin-right:0.05em;"><span class="pstrut" style="height:2.5795em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⊆</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.2222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span><span class="tag"><span class="strut" style="height:1.2425em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i:i+b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>b</mi></mrow></msub></mrow><annotation encoding="application/x-tex">y_{i:i+b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 表示一个 batch 的数据集，最后通过聚合函数将编号为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的 worker 每轮计算结果更新给整个 model 参数的梯度：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>g</mi><mo>=</mo><mi>a</mi><mi>g</mi><mi>g</mi><mi>r</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>o</mi><mi>r</mi><mo stretchy="false">(</mo><msub><mi>g</mi><mrow><mn>1</mn><mo>:</mo><mi>w</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">g=aggregator (g_{1: w})
\tag{2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">gg</span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>XGBoost 也是水平划分训练数据，计算梯度用来构建称为梯度直方图的数据结构，最终聚合梯度直方图</p>
<h3 id="Vertical-Partition">Vertical Partition</h3>
<p>垂直划分则不同，是将数据的 features 划分开来，一般每个 worker 中包含某个 feature 的全部数据，由于每个 data instance 一般都是多个 features 和一个 label，所以 label 会被复制多份在不同 worker 里。这样导致计算目标函数的难度增大，因为需要将不同 worker 上的参数权重和 feature 之积 dot product 汇总一起再去和 label 值比较计算 loss</p>
<div class="note info no-icon simple"><p><strong>dot product</strong></p>
<p>机器学习模型通过计算模型参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> 和训练实例 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 之间的点积，以获得损失 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>⋅</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">\theta \cdot x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></p>
</div>
<p>一个 worker 上提供的点积值为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>θ</mi><mo>⋅</mo><mi>x</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><msub><mi>θ</mi><mrow><mi>w</mi><mi>l</mi><mo>:</mo><mi>w</mi><mi>r</mi></mrow></msub><mo>⋅</mo><msub><mi>x</mi><mrow><mi>w</mi><mi>l</mi><mo>:</mo><mi>w</mi><mi>r</mi></mrow></msub></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(3)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\theta \cdot x = \sum_{w=1}^{W}\theta_{wl:wr}\cdot x_{wl:wr}
\tag{3}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">wl</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">wl</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">3</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>垂直分区在减少通信方面的有效性已经有人验证过，这或许也是一个优势以及优化 Distributed ML system 计算效率的方式。</p>
<h2 id="Model-Parallelism">Model Parallelism</h2>
<p>模型并行性是为单个机器不可行或单机执行时效率低下的大模型设计的，其中每个 worker 都可以访问整个 training dataset，但是每个 worker 只可以访问一部分模型参数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130000008.png" alt="Pasted image 20230130000008"></p>
<p>模型并行的 Motivation 是从需求而来的，图片转化为数据进行深度学习时参数就很多了，很多深层神经网络参数可以说是海量，在单机下根本带不动甚至存不下，例如 AlexNet 有超过 6000 万个参数，GPT-3 的完整版本有超过 1750 亿个参数。当单个 GPU 无法存储整个模型参数时，人们自然想到将模型参数划分为多个 GPU。</p>
<div class="note info no-icon simple"><p><strong>AlexNet 中参数为什么这么多</strong></p>
<p>AlexNet是一种深度卷积神经网络，其中包含8层，每一层都有许多参数。这些参数包括卷积核的大小、步长、填充、激活函数的参数以及全连接层的权重。AlexNet有60多万个参数，这使得它能够学习到图像中微妙的特征，从而达到准确地识别目标的目的，下图是AlexNet网络结构图：<br>
<img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130000610.png" alt="Pasted image 20230130000610"></p>
</div>
<p>模型并行还可以用于传统 ML 算法，例如 <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/2020408.2020426">Large-scale matrix factorization with distributed stochastic gradient descent | 17th ACM SIGKDD</a> 这篇文章提出大规模矩阵分解 (MF, Matrix Factorization) 算法：将高维 <strong>User-Item 交互矩阵</strong> D 分解为两个低维矩阵的乘积 user 矩阵 U 和 item 矩阵 V：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130000947.png" alt="Pasted image 20230130000947"></p>
<div class="note info no-icon simple"><p><strong>矩阵分解</strong></p>
<p>矩阵分解 MF 是一种数学手段，用于将一个大矩阵分解成一系列小矩阵。它可以用来分析复杂的数据集，帮助我们理解和发现其中的隐藏信息。它还可以用于求解复杂的数学方程，并进行机器学习应用。同时上面图片也能看到，将矩阵分解后，存储两个因子矩阵要比存储原本的矩阵消耗的存储空间小</p>
</div>
<p>该 MF 算法从 D 中读取元素后进行分解，分解得到的参数存储在 U 和 V 中，分解过程中的一个特性是模型参数（U 和 V）中的相同维度不能由两个并行计算单元同时更新。因此，如果 D 中的两个数据分区具有重叠的行或列（图 2.6a 和 b），则在写入相同的参数 (U 和 V) 时可能会导致冲突。考虑到这一点，该算法使用了并行策略：</p>
<ul>
<li>在每次迭代中，数据矩阵被划分为不重叠的 partition</li>
<li>每个 worker 处理一个数据分区，以便模型更新没有冲突</li>
<li>使用不同的 data partition 进行下一次迭代</li>
</ul>
<p>总的来说，模型并行最大优点是解决了 Model 过大单机无法运行的问题，主要缺点是每个工作人员都需要访问整个 dataset ，这在很多情况下其实是不可行的，数据比模型大一般是常见的情况吧，所以数据并行还是更流行一些</p>
<h2 id="Hybrid-Parallelism">Hybrid Parallelism</h2>
<p>数据并行性和模型并行性都显示了它们在训练分布式 ML 模型方面的优点，一个自然的直觉是我们可以将它们结合起来，从两个方面获得好处。混合并行是这样一种策略，它对不同类型的场景具有灵活性。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130112820.png" alt="Pasted image 20230130112820"></p>
<p>例如，其中一个 worker 使用训练数据的 subset 计算模型参数的 subset 的梯度，而其他 worker 计算其余的模型参数和不同的 data partition</p>
<h1>Parameter Sharing</h1>
<p>一个分布式机器学习框架需要聚合所有 worker 在本地计算后提出的梯度，并将各个 worker 上的本地部分模型参数 (parameter patrtition) 更新。这个聚合和更新阶段需要一个跨 worker 的模型共享机制，以保持各个 worker 上本地部分的模型参数的一致性，这样才能起到和单机训练一样的效果。</p>
<p><u>如果无法做到一致性会怎样，比如有一些 stragglers ？</u>那这个分布式计算结果就和本地单机计算结果不一致了，那这种延后性真的就全是坏处吗，有没有一些神经网络需要延后性的数据，比如有的梯度更新算法不是需要 history gradient 吗，还是说不一致可能导致模型不收敛？</p>
<h2 id="Shared-Nothing">Shared Nothing</h2>
<p>模型共享的一种方式是让每个工作人员通过网络通信维护模型副本以及梯度聚合，称为 Share Nothing Architecture。换句话说，没有存储模型参数全局状态的中心节点。下图就是一个 SN 架构的模型示意。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130114335.png" alt="Pasted image 20230130114335"></p>
<p>SN 架构下每个 node 都有自己独立的处理器、存储、内存等资源，这些资源在 node 之间不进行共享，这样的好处是便于添加新的 node，毕竟不用考虑带给其他 node 的影响。SN 架构的一个典型实例是在数据库中进行分片，其中一个表被划分为一组 nodes ，每个 node 都可以使用相同的数据模式独立工作。</p>
<div class="note info no-icon simple"><p><strong>分布式计算中 node 和 worker</strong></p>
<p>Node 和 Worker 之间的区别在于，Node 是一种分布式计算系统中的一个节点，它可以运行多个应用程序，接收和处理客户端请求。Worker 是 Node 的子进程，它们负责执行具体的任务。另外，Node 还可以通过远程通信来与其他节点进行通信；而 Worker 则只能在本地运行，不能与其他节点通信。本节之前说的worker其实更算是node，当然，一个node执行一个worker也是可以的，这个时候就没啥区别了。</p>
</div>
<p>分布式机器学习 DML 中的数据天然满足 SN 架构的定义：训练数据和模型参数在 worker 之间进行分区，每个 worker 都有专用的硬件资源。分布式机器学习自身的特点也有一些，比如迭代优化算法需要工人之间的数据聚合，这可以通过一些 communication infrastructures 实现，所以通信问题在 DML 中的重要性可见一斑。</p>
<p>在 DML 三种常用的通信框架为：<strong>MPI</strong> (Message Passing Interface), <strong>RPC</strong> (Remote Procedure Call), and <strong>MapReduce</strong>.</p>
<h3 id="MPI-Message-Passing-Interface">MPI (Message Passing Interface)</h3>
<p>MPI 是消息传递接口的缩写，是一种相对低级的编程 API，在并行计算中被广泛采用，可以在分布式进程之间进行通信。MPI 因其高性能、可扩展性和可移植性而被业界和学术界广泛采用。</p>
<p>MPI 消息传递接口 (Message Passing Interface, MPI) 是一种用于在计算机网络上进行并行计算的标准应用程序接口，它使得各个处理器可以在分布式计算机集群中进行通信。</p>
<p>在 MPI 的编程模型中，一个重要的概念是 <strong>communicator</strong>，它定义了一组可以相互通信的进程，在 <strong>communicator</strong> 中的每个过程都有一个等级作为唯一标识。MPI 的基本点到点接口是 MPI_Send 和 MPI_Recv 运算符，一个进程可以通过 MPI_Send 运算符将数据发送到另一个进程 (有等级标识) ，也可以通过 MPI_Recv 操作符从特定进程接收数据。一些 MPI 的实现框架也提供了用于分布式通信的集合操作符，这些集合操作符涉及通信器组中所有过程之间的通信，例如 MPI_Bcast, MPI_Allreduce, MPI_Reduce 等等</p>
<p>下面是一个使用 MPI 消息传递接口实现的并行计算的例子，假设我们要实现一个并行计算，要求所有处理器都要向其它处理器发送一条信息：</p>
<ul>
<li>
<p>使用 MPI 函数 <code>MPI_Init()</code> 初始化 MPI 环境；</p>
</li>
<li>
<p>使用 <code>MPI_Comm_size()</code> 和 <code>MPI_Comm_rank()</code> 获取当前处理器的数量和标识；</p>
</li>
<li>
<p>之后循环遍历所有处理器：对于当前处理器而言，如果它不是最后一个处理器 (rank != size-1), 则向下一个处理器发送信息；如果当前处理器是最后一个处理器（rank == size-1）, 则不再发送信息。这里我们使用了 <code>MPI_Send()</code> 函数来发送信息。最后，我们使用 <code>MPI_Finalize()</code> 函数来释放 MPI 资源。</p>
</li>
</ul>
<p>其他常用的集合操作符如下表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:left">Collective Operator</th>
<th style="text-align:left">Function</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">MP_Bcast</td>
<td style="text-align:left">The root rank (worker) broadcasts its local data to all other ranks (workers).</td>
</tr>
<tr>
<td style="text-align:left">MP_Scatter</td>
<td style="text-align:left">The root rank partitions its local data item and sends one portion to each rank independently.</td>
</tr>
<tr>
<td style="text-align:left">MPI_Gather</td>
<td style="text-align:left">The root rank collects all the data items from other ranks, yielding an array of data items</td>
</tr>
<tr>
<td style="text-align:left">MPI_Redruce</td>
<td style="text-align:left">The root rank collects all the data items from other ranks and sums them together</td>
</tr>
<tr>
<td style="text-align:left">MPI_Allgather</td>
<td style="text-align:left">Instead of gathering the data items on the root rank, MPI_Allgather gathers the data items on every rank</td>
</tr>
<tr>
<td style="text-align:left">MPI_Allreduce</td>
<td style="text-align:left">Similarly, all the ranks perform the reduce operation and obtain the sum of all data items</td>
</tr>
<tr>
<td style="text-align:left">MPI_AllToAll</td>
<td style="text-align:left">Similarly, all the ranks perform the reduce operation and obtain the sum of all data items</td>
</tr>
<tr>
<td style="text-align:left">MPI_Barrier</td>
<td style="text-align:left">To schedule different ranks, MPI_Barrier sets a barrier which is a synchronization point for all the ranks</td>
</tr>
</tbody>
</table>
<p>下图是每种集合操作符功能的图像示意，多色方块代表数据，圆圈是 worker，其中数字代表 rank</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130141054.png" alt="Pasted image 20230130141054"></p>
<p>OpenMPI 结合了其他几个项目（FT-MPI、LA-MPI、LAM/MPI 和 PACX-MPI）的技术和资源，mpi4py 为 Python 编程语言提供了 MPI 标准的绑定，允许任何 Python 程序使用多个处理器进行计算。MPICH 是 MPI 标准的高性能和广泛可移植的实现，其衍生产品是世界上使用最广泛的 MPI 实现，特别是对于超级计算机。</p>
<h3 id="RPC-Remote-Procedure-Call">RPC (Remote Procedure Call)</h3>
<p>远程过程调用，简称 RPC，是一种请求-响应消息传递协议。RPC 建立了一个服务器-客户端框架，在该框架中，服务器公开其本地过程 API，客户端可以远程调用服务器的 API。RPC 的目标是让远程服务器客户端通信像本地过程调用一样运行。RPC 由于其<strong>多语言支持、跨平台部署、可移植性和灵活性</strong>，促进了应用程序的开发。RPC 的编程是用户友好的，因为它将与底层操作系统和网络相关的实现细节与开发人员隔离开来。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130144106.png" alt="Pasted image 20230130144106"></p>
<p>上图中是一个 RPC 过程的生命周期示意图，整个流程可以认为有 6 个步骤：</p>
<ol>
<li>客户端进程调用 RPC 函数并将所需参数打包到消息中。此打包操作称为封送处理</li>
<li>消息作为 RPC 请求发送到服务器</li>
<li>服务器进程接收 RPC 请求并从消息中解组参数</li>
<li>触发 RPC 处理程序以执行主要功能</li>
<li>RPC 处理程序的返回结果被封送并作为 RPC 响应发送到客户端</li>
<li>客户端进程接收 RPC 响应，解组消息，并将结果返回给用户</li>
</ol>
<p>一个经典的 RPC 框架是 ApacheThrift，它是一种轻量级的、独立于语言的软件，用于点对点 RPC 实现。Thrift 为数据传输、数据序列化和应用程序级处理提供了干净的抽象和实现。Thrift 提供了一种自动代码生成工具，它以简单的定义语言作为输入，跨编程语言生成代码，并使用抽象堆栈构建可互操作的 RPC 客户端和服务器</p>
<h3 id="MapReduce">MapReduce</h3>
<p>MPI 和 RPC 虽然在工业界和学术界都被广泛采用，但都是低级编程 API。它们仍然需要相对较高的编程障碍。从业者需要了解设计并行计算和处理系统相关问题的基本技术，例如位置、调度、系统故障和 stragglers 等问题，所以 MapReduce 之类的框架就在解决易用性问题。</p>
<p>关于 MapReduce 详细原理参考论文阅读笔记：[[MapReduce - Paper Reading]]</p>
<p>下面讲讲在 DML 中 MapReduce 是如何工作的。通过 map 函数可以并行计算大规模数据的 statistics（如 gradient） ，通过 reduce 函数可以实现 statistics 的聚合，下图中的输入是训练数据和初始模型参数：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130234726.png" alt="Pasted image 20230130234726"></p>
<p><strong>Map 阶段</strong>：每个 worker 上的 map 函数以模型参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 及其对应的数据分区 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>X</mi><mi>i</mi><mtext>，</mtext><mi>Y</mi><mi>i</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(Xi，Yi)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord mathnormal">i</span><span class="mord cjk_fallback">，</span><span class="mord mathnormal">Yi</span><span class="mclose">)</span></span></span></span> 作为输入，使用模型参数和训练数据计算诸如梯度的中间值</p>
<p><strong>Reduce 阶段</strong>：由所有 worker 计算的局部梯度被聚集以获得生成新模型参数的全局梯度，该梯度用于更新模型参数，下一次迭代继续使用新的模型参数</p>
<p>MLlib 是 Apache Spark 的可扩展机器学习库，它涵盖了广泛的机器学习模型，例如分类、回归、聚类、决策树、推荐、主题模型和 ML pipline</p>
<h2 id="Shared-Memory">Shared Memory</h2>
<p>共享内存架构选择建立所有 worker 都可以访问的内存存储，他们之间可以通过互连网络在共享存储器中读写共享数据</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230130235329.png" alt="Pasted image 20230130235329"></p>
<p>共享内存架构有两种方式，一种是在单机器中多核进行 Machine Learning 计算，另一种是多机分布式计算，一般研究 DML 的人专注于后者。</p>
<p>共享内存架构的 Motivation 是解决 SN 架构的缺点：MPI 和 RPC 为开发人员带来了太多复杂的通信细节，而 MapReduce 虽然有用户友好的 API 以及能隔离通信细节，降低编程门槛，但是在面临海量数据时能力不足。</p>
<p>MapReduce 能力不足主要体现在：早年的数据的 size 和 dimension 都处于相对低的 level，但是现在数据参数很多，Machine Learning 的过程中产生的 statistics 大多是高维度 vector 或者 matrix，在使用一个 worker 进行 Reduce 操作来聚合这些 statistics 时，受限于网络带宽，该 worker 很容易成为整个 MapReduce 的 bottle neck，为了解决这个问题，设置几个服务器用来专门聚合参数，这样的方式是 SM 架构，也叫参数服务器架构，示意图如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230131182103.png" alt="Pasted image 20230131182103"></p>
<p>在参数服务器架构中每个参数服务器存储一部分模型参数，图中上面的若干个 server 中不同颜色方块代表不同的 intermediate statistic，这些 servers 共同管理这些 statistic 并用它们更新 parameter，它们暴露给 worker 的 API 主要有 pull 和 push，分别用于提交 intermediate statistic 和拉取参数。</p>
<p><u>每个 worker 上是有完整的模型的，但 local dataset 是 partition 的</u>，每个 worker 利用其上的 local dataset 计算出特定参数的 statistics，worker 将这些不同类型的 statistics 传递给不同的 server 用于更新总参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>，需要新模型参数时调用 pull 接口拉取。被传递的 statistic 可以是本地模型参数或者全局模型参数，也可以是参数的 update，以及 gradient 。</p>
<p>可以看到，参数的更新压力被分散到了海量 workers 中去，根据 statistics 更新部分参数并聚合后更新模型总参数的任务交给了 parameter servers。</p>
<p><strong>Advantages</strong></p>
<p>参数服务器模型对于大规模高纬度的数据有很大优势。当数据集扩大的时候添加更多的 worker 即可，更多的 worker 可以训练更多的数据；如果数据维度增加，这意味着参数（weight）也要增加，由于不同 server 负责不同的参数，所以添加新的 server 即可。</p>
<h1>Synchronization</h1>
<p>基于机器学习的训练机制，在进行下一轮迭代前要将模型参数聚合并使得 workers 中都更新为全局版本后才可以进行下一轮迭代。<u>这需要 DML 系统能够顺利实现参数聚合以及对于 worker 的跨迭代轮次调度</u>。</p>
<p><u>另一个挑战是分布式系统中不同 worker 所处的软硬件环境具有异质性，由于硬件设备的不同，机器的计算和通信能力也不同</u>。即使机器配备相同硬件，由于意外的系统运行时的异常或网络拥塞，它们的执行能力仍然会有所不同。现在边缘计算中的云边协同设计也是面临这样的问题，出于通信原因，一部分计算放在 edge 端，但是 edge 端不同设备之间算力并不相同，整个分布式系统的全局优化问题和 edge 端每个 worker 上训练时分配的 partition dataset size 也有一些关系，有这方面的研究在做。</p>
<p>所以需要设计好同步协议，根据对延迟的容忍度和收敛灵活性，有各种各样的同步协议，从纯同步到纯异步的都有。总的来讲有三种：批量同步协议（BSP）、异步协议（ASP）和过时同步协议（SSP）。</p>
<h2 id="Bulk-Synchronous-Protocol">Bulk Synchronous Protocol</h2>
<p>在 BSP 下，每个 worker 维护模型参数的副本，并使用分配的 data partition 来计算局部梯度。在每轮迭代结束时都有一个 barrier 用于强制每个工作人员停止运行并等待其他工作人员。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230201225919.png" alt="Pasted image 20230201225919"></p>
<p>BSP 已在最流行的分布式机器学习系统中实现，如 Mahout、MLlib、以及 TensorFlow 和 PyTorch。主要有三个优点：1. 适用于机器学习模型的大多数优化算法，包括广义线性模型、聚类、树模型和深度学习模型；2. BSP 的逻辑简单易实现；3. BSP 保证与优化算法的顺序执行等效，因此保证理论收敛。</p>
<p>然而，BSP 在 worker 之间具有很强的依赖性，一次迭代的执行必须等待前面一次迭代完成。在异构分布式环境中，由于不同的计算能力、网络带宽或不可预测的拥塞，worker 的执行速度会有所不同。因此，worker 通常需要不同的时间成本来运行迭代。这导致了一个 straggler 的问题，即有些 worker 跑得快，而另一些 worker 跑得慢，快的不得不等待慢的，浪费了资源。</p>
<h2 id="Asynchronous-Protocol">Asynchronous Protocol</h2>
<p>为了解决在异构分布式环境下 BSP 的痛点，Asynchronous Protocol （ASP） 没有使用同步障碍 barrier ，采用一种尽力交付 best-effort 策略，因此 worker 不需要彼此等待。</p>
<p>参数服务器模型是支持同步 BSP 和异步 ASP 协议的，以参数服务器架构下的 SGD 优化方式来看 ASP 的特点：</p>
<ol>
<li>每个 worker 使用一小批训练数据计算局部梯度</li>
<li>每个 worker 在每轮计算结束后对本地计算的 local gradients 进行划分，并将梯度不同部分推送到对应的 parameter server 中</li>
<li>每个 parameter server 接收来自 worker 的梯度，并根据 SGD 的规则更新存储的模型参数</li>
<li>每个 worker 从参数服务器中提取最新的模型参数，不等待其他 worker，直接开始下一次迭代</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230203205212.png" alt="Pasted image 20230203205212"></p>
<p>然而，由于 ASP 允许不同 worker 的处理速度以及本地模型副本之间的不一致性，ASP 无法保证梯度优化算法的正确收敛，有时甚至导致优化的发散。</p>
<p>看到这，ASP 这一点和前面自己的疑惑基本一致，不同步的方案有用但有缺点，不过在这个思路基础上进行的相关工作还是挺多的。这也感觉到看这本书的价值了，基本上是结合知识点进行讲解的一个综述，核心概念比较简单，但是在一些基本思想上实现一个可落地系统方案，构成一篇文章，无论是细节设计、理论支撑与计算，还是代码难度等等应该都有很多需要解决。</p>
<h2 id="Stale-Synchronous-Protocol">Stale Synchronous Protocol</h2>
<p>由于硬件异构、硬件故障、数据不平衡、资源共享和网络延迟，straggler 现象非常普遍。ASP 通过完全消除同步障碍来解决系统异构性，但引入了不稳定的收敛。</p>
<p>另一种解决方案是在 BSP 和 ASP 之间进行 trade off，为同步引入一定的灵活性，而不是完全消除障碍，这类协议属于 SSP 。有一说一，太经典了，遇事不决 trade off 哈哈😂。</p>
<p>与 BSP 和 ASP 不同，SSP 的同步规则是最快的 worker 和最慢的 worker 之间的最大迭代间隔不能超过预定义的阈值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span>。因此，不在每次迭代时设置固定的 barrier 而是根据最快和最慢进程动态调整，当差距到达阈值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 时最快的 worker 需要等待最慢的 worker 。在 SSP 下，每个工作程序的处理与 ASP 相同，不同之处在于参数服务器的实现：</p>
<ol>
<li>parameter 需要监控全部 workers 的迭代轮次</li>
<li>当 parameter 接收到 worker push 操作推来的 statistic 时直接更新模型，不用聚合完再更新了</li>
<li>当一个 worker 试图从 parameter server 中提取最新的模型参数时，server 首先检查工作人员的当前迭代。如果允许 worker 根据 SSP 的规则（即看 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span>）处理下一次迭代，则 server 将向该 worker 发送最新的模型参数。否则，如果 worker 的请求太快，该 server 会挂起该 worker 的请求，并等待 SSP 条件得到满足</li>
</ol>
<p>基于 SSP 的 ESSP 利用了一种更积极的策略，一旦 parameter server 上的模型参数更新，server 就会主动向 worker 推送最新的模型参数，这种方案减小了 local 和 global 参数的差异，以额外的通信成本为代价，加速了梯度优化算法的收敛。</p>
<p>SSP 这类协议限制了 worker 之间的工作速度差距（例如，迭代次数、训练轮数 epoch、使用的批次数 batch number）。在模型更新方面，它们的工作方式与 ASP 相同，将 intermediate statistics 推送到 parameter server。与 BSP 和 ASP 相比，SSP 类型的方法可以减少 BSP 中 stragglers 等待时间，并通过限制 ASP 无法保证的速度差距来提供理论上的收敛保证。</p>
<p>但是，SSP 类方法中局部模型参数和全局模型参数之间仍然存在不可避免的不一致性问题。选择较小的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 阈值可以减少这种不一致性，但会增加等待时间。相比之下，选择较大的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 阈值可以减少等待开销并增加模型不一致性。因此，选择一个合适的阈值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> 就成了一个难题，不同的 task 要根据实际 workload 来调参。</p>
<h1>Communication Optimization</h1>
<p>当计算速度难以提高时，优化通信是加速执行的有效方法，这可以通过压缩传输的数据（例如，梯度）或丢弃不太重要的数据项来实现。</p>
<h2 id="Lower-Numerical-Precision">Lower Numerical Precision</h2>
<p>高精度数据表示适用于需要输出可再现的易出错任务。然而许多机器学习模型在训练过程中对模型参数和 intermediate statistic 容忍度比较高。甚至一些研究工作表明，引入噪声可以提高机器学习模型的性能，特别是对于非凸深度学习模型，该模型添加一些噪声梯度可能有利于跳出局部最优。基于此，可以使用较少的比特来表示模型参数和/或用于计算和通信的中间统计信息，这种思路叫做量化，有三个好处：</p>
<ol>
<li>低精度数字的计算速度比高精度数字的更快</li>
<li>较低精度的计算消耗较少的内存占用</li>
<li>低精度数据通信成本较低</li>
</ol>
<p>注意一种数据格式：定点数，是实数的一种数据类型，它在基数点之后（有时也在基数点之前）有固定位数。与浮点格式相比，定点格式更适合没有 FPU（浮点单元）的处理器，例如嵌入式微处理器和微控制器。</p>
<p>许多现有工作已经探索了训练机器学习模型时使用较低精度数据的可能性，无论是算法设计方面，还是从硬件为实现低精度格式提供基础设施，例如训练引擎为低精度计算提供了精心设计的 API，TensorFlow、PyTorch、MXNet 和 NumPy，提供了易于使用的半精度编程接口。</p>
<h2 id="Communication-Compression">Communication Compression</h2>
<p>考虑压缩之前要注意，压缩起到的效果和压缩算法本身耗费之间有 trade off，又简单又有效的压缩算法当然是最好的</p>
<p>较低精度的数据表示形式可以减少内存占用，加速计算，并以计算过程中引入的错误为代价减少通信开销。为了优化通信，另一种方法是在传输 intermediate statistics 之前对其进行压缩。数据压缩是一个重要研究领域，当数据存储和网络带宽成为瓶颈时一般可以考虑数据压缩，其目标是使用比原始数据表示更少的比特来 encode （或压缩）信息，并在 decode （或解压缩）后保留数据信息。</p>
<p>传统整数压缩方案有很多成熟的，但是 ML 并不能使用，因为 ML 使用浮点数，<u>但是了解它们是必要的，因为为 ML 设计的一些压缩算法借鉴了整数的压缩</u></p>
<div class="mermaid-wrap"><pre class="mermaid-src" hidden>
   
flowchart LR
A(Data Compress)--&gt;B(For Integer)
B--&gt;B1(RLE)
B--&gt;B2(Huffman)
B--&gt;B3(Rice)
A--&gt;C(But ML uses float numbers)
  </pre></div>
<p>根据压缩前后数据损失可以分为有损和无损。无损压缩（lossless）通过消除统计冗余减少数据大小，同时不丢失任何信息。相反，有损压缩（lossy）通过删除不必要的和不太重要的信息来缩小数据大小，同时在解压缩后可能会丢失信息。</p>
<h3 id="Lossless-Compression-for-Integer-Numbers">Lossless Compression for Integer Numbers</h3>
<p><strong>RLE Run-length Encoding</strong></p>
<p>Run-length Encoding，行程编码算法，是无损的，原理很简单，将连续出现的数据用数据单元和频次表示出来，假设原始数据包含一段连续重复项目 AAAAA BBB，RLE 将其编码为 occurence + data item 的组合后为 5A3B。RLE 可用于图像的压缩，因为图像中许多相邻像素是相同的。</p>
<p><strong>Huffman</strong></p>
<p>霍夫曼编码，比较经典的算法。思路是高频 item 使用更短的代码，低频 item 使用更长的代码。通过这种方式，霍夫曼编码后的预期平均长度被最小化。算法实现需要建立一个二叉树，树的大小由需要编码的 item 决定。</p>
<h3 id="Lossless-Compression-for-Sparse-Matrices">Lossless Compression for Sparse Matrices</h3>
<p>一些数据存储格式为稀疏矩阵/向量提供了空间效率优化，可以在机器学习工作负载中压缩数据表示而不会丢失。</p>
<p><strong>COO</strong></p>
<p>COO 不存那些冗余的 0 值，利用三元组存储非零元素的位置和值，COO 将这些三元组存储在三个数组中，即行索引数组、列索引数组和值数组</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230203154649.png" alt="Pasted image 20230203154649"></p>
<p>实际上可以看到对小矩阵或者没那么稀疏的矩阵而言，这种压缩还不如不压缩</p>
<p><strong>CSR</strong></p>
<p>压缩稀疏行（CSR）是一种更有效的稀疏矩阵存储格式，CSR 也存储三种类型的数据：非零数值、列索引和行偏移。</p>
<p>行偏移数组：存储矩阵每行的偏移量，存的是每一行中第一个非零数字在 values 数组中的下标位置（下标从 0 算起），所以 row offsets 结果是 0135，分别对应 1、9、3、5 这四个每行中的第一个非零字母在 values 数组中的下标，而 6 表示整个 matrix 中的非 0 元素个数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230203155741.png" alt="Pasted image 20230203155741"></p>
<p>CSR 可以显著减少存储空间。有研究表明 CSR 格式每一个非零 32 位浮点元素平均花费 8.5 字节，每一个 64 位浮点元素花费 12.5 字节。然而，由于其复杂的编码方案，CSR 需要比 COO 更多的计算成本。</p>
<p><strong>CSC</strong></p>
<p>压缩稀疏列（CSC）是一种列优先的稀疏存储格式。CSC 与 CSR 类似，只是值按列存储。这里 column offsets 的结果是 0135，对应也是 1、9、3、5 这四个每行中的第一个非零字母在 values 数组中的下标，可以看到无论是 row offset 还是 column offset 的结果都是一致的，这是因为两种方案的 values 数组计数方式也根据按行还是按列进行调整了。</p>
<p><img src="https://cdn.jsdelivr.net/gh/hyattdd/cloud_img/blog/Pasted%20image%2020230203172609.png" alt="Pasted image 20230203172609"></p>
<h3 id="Lossy-Compression-for-Floating-point-Numbers">Lossy Compression for Floating-point Numbers</h3>
<p>上面几种方式确实是无损压缩，然而，对于许多基于梯度的优化算法，传输的浮点中间统计信息（通常是梯度）是密集的，压缩稀疏矩阵的算法不太适合。所以一些相关算法提出了一些有损失的压缩方式用于传递梯度。</p>
<p>主要有两个思路，一是梯度量化降低梯度的精度，二是梯度稀疏化，将梯度的维度减少。</p>
<p><strong>Gradient Quantization</strong></p>
<p>梯度量化是一类利用量化策略根据原始数据的值范围将浮点数转换为整数的算法。为什么量化之后的梯度还能够让模型收敛到比较准确的效果？本质上，梯度是向量。而一个向量最重要的无外乎两点——方向和大小。方向指明了接下来的走向，而大小指明了走的步幅。在 SGD 中，我们可以很自然接受这样一个事实：大多时候梯度的方向比其大小更加重要。那么，如果我们对梯度的大方向不做很大的修改，而只是在其大小上做整齐的规划。那么会得到什么效果呢？我们只需要很少的比特就能表示梯度所表达的信息。进一步我们自然而然就可以减少通信开销。</p>
<p>量化这块感觉还是挺有意思的，涉及到编码之类的知识，以前听室友提过，以为仅仅是舍弃数据精度，还寻思为什么要专门有人研究这个。现在发现这本书里提到的量化方法还是比较复杂和严谨的。</p>
<p><strong>Gradient Sparsification</strong></p>
<p>梯度稀疏化是指：通过只发送梯度向量中的重要分量并丢弃不太重要的分量来对梯度向量进行稀疏化。梯度稀疏化有多大的必要性呢？</p>
<div class="note info no-icon simple"><p>Lin et al. find that 99.9% of the gradient exchange in distributed SGD is redundant. They propose Deep Gradient Compression (DGC) to greatly reduce communication cost . To preserve accuracy during this compression, DGC employs four techniques: momentum correction, local gradient clipping, momentum factor masking, and warm-up training.</p>
</div>
<p>震惊了属于是，百分之 99 的都是非必要传输，那么确实有很大的压缩空间。</p>
<h1>Reference</h1>
<p><a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-981-16-3420-8">Distributed Machine Learning and Gradient Optimization | SpringerLink</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94848941">三值梯度量化-知乎</a></p>
<br></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Distributed-ML/">Distributed ML</a></div><div class="post_share"><div class="social-share" data-image="/./img/touxiang.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Hyatt.D</div><div class="author-info__description">Seek truth from the facts.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">47</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">11</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HyattDD"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HyattDD" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:tjudht@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Basics of Distributed Machine Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Anatomy of Distributed Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Computing-Parallelism"><span class="toc-number">2.0.1.</span> <span class="toc-text">Computing Parallelism</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameter-Aggregation"><span class="toc-number">2.0.2.</span> <span class="toc-text">Parameter Aggregation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Worker-Synchronization"><span class="toc-number">2.0.3.</span> <span class="toc-text">Worker Synchronization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Communication-Optimization"><span class="toc-number">2.0.4.</span> <span class="toc-text">Communication Optimization</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">Parallelism</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Data-Parallelism"><span class="toc-number">3.1.</span> <span class="toc-text">Data Parallelism</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Horizontal-Partition"><span class="toc-number">3.1.1.</span> <span class="toc-text">Horizontal Partition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Vertical-Partition"><span class="toc-number">3.1.2.</span> <span class="toc-text">Vertical Partition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Parallelism"><span class="toc-number">3.2.</span> <span class="toc-text">Model Parallelism</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hybrid-Parallelism"><span class="toc-number">3.3.</span> <span class="toc-text">Hybrid Parallelism</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">Parameter Sharing</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Shared-Nothing"><span class="toc-number">4.1.</span> <span class="toc-text">Shared Nothing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MPI-Message-Passing-Interface"><span class="toc-number">4.1.1.</span> <span class="toc-text">MPI (Message Passing Interface)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RPC-Remote-Procedure-Call"><span class="toc-number">4.1.2.</span> <span class="toc-text">RPC (Remote Procedure Call)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce"><span class="toc-number">4.1.3.</span> <span class="toc-text">MapReduce</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Shared-Memory"><span class="toc-number">4.2.</span> <span class="toc-text">Shared Memory</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">Synchronization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Bulk-Synchronous-Protocol"><span class="toc-number">5.1.</span> <span class="toc-text">Bulk Synchronous Protocol</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Asynchronous-Protocol"><span class="toc-number">5.2.</span> <span class="toc-text">Asynchronous Protocol</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stale-Synchronous-Protocol"><span class="toc-number">5.3.</span> <span class="toc-text">Stale Synchronous Protocol</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">Communication Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Lower-Numerical-Precision"><span class="toc-number">6.1.</span> <span class="toc-text">Lower Numerical Precision</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Communication-Compression"><span class="toc-number">6.2.</span> <span class="toc-text">Communication Compression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Lossless-Compression-for-Integer-Numbers"><span class="toc-number">6.2.1.</span> <span class="toc-text">Lossless Compression for Integer Numbers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lossless-Compression-for-Sparse-Matrices"><span class="toc-number">6.2.2.</span> <span class="toc-text">Lossless Compression for Sparse Matrices</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lossy-Compression-for-Floating-point-Numbers"><span class="toc-number">6.2.3.</span> <span class="toc-text">Lossy Compression for Floating-point Numbers</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">Reference</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/11/Using%20CUDA%20on%20WSL2/" title="GPU Programming Basic Knowledge">GPU Programming Basic Knowledge</a><time datetime="2023-07-11T01:00:00.000Z" title="Created 2023-07-11 09:00:00">2023-07-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/06/Paper%20Reading%20Note-Pollux/" title="Paper Reading Note of OSDI2021 Best Paper - Pollux">Paper Reading Note of OSDI2021 Best Paper - Pollux</a><time datetime="2023-07-06T00:00:00.000Z" title="Created 2023-07-06 08:00:00">2023-07-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/23/Simple%20Guide%20of%20Bazel/" title="Simple Guide of Bazel">Simple Guide of Bazel</a><time datetime="2023-06-23T11:00:00.000Z" title="Created 2023-06-23 19:00:00">2023-06-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/23/Recsys/" title="Recommend System Based on Bipartite Graph">Recommend System Based on Bipartite Graph</a><time datetime="2023-04-23T11:00:00.000Z" title="Created 2023-04-23 19:00:00">2023-04-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/03/27/GNN%20Intro/" title="GNN Background Knowledge">GNN Background Knowledge</a><time datetime="2023-03-27T12:00:00.000Z" title="Created 2023-03-27 20:00:00">2023-03-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Hyatt.D</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>